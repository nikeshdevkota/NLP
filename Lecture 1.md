# Natural Language Processin
## Lesson 1

<h3>Word Vector</h3> 


* Word vectors are mathematical representations of words used in NLP tasks. 
* They capture the semantic meaning of words by mapping each word to a high-dimensional vector in a continuous vector space, based on the context in which the word appears. 
* Similar words are mapped to vectors that are close together in the vector space, while dissimilar words are farther apart. 
* Word vectors are used in various NLP tasks such as text classification, sentiment analysis, machine translation, etc.

<h3>Examples of Word Vectors</h3> 

* Word2Vec: a popular word vector representation that uses a shallow neural network to learn the relationships between words based on their co-occurrence in large text corpora.
* GloVe (Global Vectors for Word Representation): another popular word vector representation that uses a matrix factorization approach to capture the co-occurrence statistics of words in large text corpora.
* FastText: an extension of the Word2Vec approach that takes into account the subword structure of words to capture the meaning of rare and out-of-vocabulary words.
* ELMo (Embeddings from Language Models): a deep contextualized word representation that uses a pre-trained bidirectional LSTM language model to capture the context-dependent meaning of words.
